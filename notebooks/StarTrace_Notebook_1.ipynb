{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations & Imports & Constants"
      ],
      "metadata": {
        "id": "xoxsvczkyc3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Run after the Drive connection is established)"
      ],
      "metadata": {
        "id": "MlwxuEOozt1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lthpE_XeyKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/drive/My Drive/Startrace_Project_Data/requirements.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "6gaycxbtyVlG",
        "outputId": "654c6e90-4934-4b08-c95d-a63e761f6a54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contourpy==1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools==4.56.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 3)) (4.56.0)\n",
            "Requirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: kiwisolver==1.4.8 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 5)) (1.4.8)\n",
            "Collecting matplotlib==3.10.1 (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 6))\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy==2.2.3 (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 7))\n",
            "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging==24.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 8)) (24.2)\n",
            "Collecting pandas==2.2.3 (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 9))\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow==11.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 10)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing==3.2.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 11)) (3.2.1)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 12))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pytz==2025.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 13)) (2025.1)\n",
            "Collecting PyWavelets==1.8.0 (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 14))\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 15)) (1.6.1)\n",
            "Collecting scipy==1.15.2 (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 16))\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 17)) (0.13.2)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 18)) (1.17.0)\n",
            "Collecting threadpoolctl==3.6.0 (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 19))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tzdata==2025.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/Startrace_Project_Data/requirements.txt (line 20)) (2025.1)\n",
            "Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, python-dateutil, numpy, scipy, PyWavelets, pandas, matplotlib\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.5.0\n",
            "    Uninstalling threadpoolctl-3.5.0:\n",
            "      Successfully uninstalled threadpoolctl-3.5.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.3 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.3 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyWavelets-1.8.0 matplotlib-3.10.1 numpy-2.2.3 pandas-2.2.3 python-dateutil-2.9.0.post0 scipy-1.15.2 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              },
              "id": "2e8f4d89e3c1457483f8b98c2a7a9c24"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.fft import fft\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "wDqUzSEy0k-b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXOPLANET_DATA_FILE = \"/content/drive/My Drive/Startrace_Project_Data/cumulative.csv\""
      ],
      "metadata": {
        "id": "i_5j6wDS0oKV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "M7mTcrTIyC5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExoplanetData:\n",
        "\n",
        "    def __init__(self, file_path: str):\n",
        "        self.file_path = file_path\n",
        "        self.df = self._load_data()\n",
        "\n",
        "    def _load_data(self) -> pd.DataFrame:\n",
        "        df = pd.read_csv(self.file_path, comment='#')\n",
        "        return df\n",
        "\n",
        "    def show_head(self, n: int = 5):\n",
        "        print(self.df.head(n))\n",
        "\n",
        "    def summary(self) -> pd.DataFrame:\n",
        "        return self.df.describe()\n",
        "\n",
        "    def filter_by_disposition(self, disposition: str) -> pd.DataFrame:\n",
        "        return self.df[self.df['koi_disposition'] == disposition]\n",
        "\n",
        "    def get_columns(self) -> list:\n",
        "        return list(self.df.columns)\n",
        "\n",
        "    def get_dataframe_with_columns(self, columns: list) -> pd.DataFrame:\n",
        "        existing_columns = [col for col in columns if col in self.df.columns]\n",
        "        return self.df[existing_columns]\n",
        "\n",
        "    def extract_statistical_features(self, columns: list) -> pd.DataFrame:\n",
        "        feature_df = pd.DataFrame(index=self.df.index)\n",
        "        for col in columns:\n",
        "            if col in self.df.columns:\n",
        "                mean_val = self.df[col].mean()\n",
        "                std_val = self.df[col].std()\n",
        "                min_val = self.df[col].min()\n",
        "                max_val = self.df[col].max()\n",
        "                median_val = self.df[col].median()\n",
        "\n",
        "                feature_df[f'{col}_mean'] = mean_val\n",
        "                feature_df[f'{col}_std'] = std_val\n",
        "                feature_df[f'{col}_min'] = min_val\n",
        "                feature_df[f'{col}_max'] = max_val\n",
        "                feature_df[f'{col}_median'] = median_val\n",
        "        return feature_df\n",
        "\n",
        "    def extract_fourier_features(self, columns: list) -> pd.DataFrame:\n",
        "        feature_df = pd.DataFrame(index=self.df.index)\n",
        "        for col in columns:\n",
        "            if col in self.df.columns:\n",
        "                fft_values = np.abs(fft(self.df[col]))\n",
        "                feature_df[f'{col}_fft_mean'] = np.mean(fft_values)\n",
        "                feature_df[f'{col}_fft_std'] = np.std(fft_values)\n",
        "                feature_df[f'{col}_fft_max'] = np.max(fft_values)\n",
        "        return feature_df\n",
        "\n",
        "    def extract_wavelet_features(self, columns: list, wavelet='haar') -> pd.DataFrame:\n",
        "        feature_df = pd.DataFrame(index=self.df.index)\n",
        "        for col in columns:\n",
        "            if col in self.df.columns:\n",
        "                # Her satır için Wavelet dönüşümü uygula\n",
        "                coeffs = pywt.wavedec(self.df[col], wavelet, level=1)  # 1 seviyeli ayrıştırma\n",
        "                # Detay katsayılarının istatistiksel özelliklerini al\n",
        "                feature_df[f'{col}_wavelet_detail_mean'] = np.mean(coeffs[0])\n",
        "                feature_df[f'{col}_wavelet_detail_std'] = np.std(coeffs[0])\n",
        "                feature_df[f'{col}_wavelet_approx_mean'] = np.mean(coeffs[1])\n",
        "                feature_df[f'{col}_wavelet_approx_std'] = np.std(coeffs[1])\n",
        "        return feature_df\n",
        "\n",
        "    def extract_manual_features(self) -> pd.DataFrame:\n",
        "        feature_df = pd.DataFrame(index=self.df.index)\n",
        "\n",
        "        if 'koi_depth' in self.df.columns and 'koi_period' in self.df.columns:\n",
        "            feature_df['depth_period_ratio'] = self.df['koi_depth'] / self.df['koi_period']\n",
        "\n",
        "        if 'koi_steff' in self.df.columns:\n",
        "            feature_df['koi_steff_squared'] = self.df['koi_steff'] ** 2\n",
        "\n",
        "        return feature_df\n",
        "\n",
        "\n",
        "    def show_target_distribution(self, target_column: str):\n",
        "        if target_column in self.df.columns:\n",
        "            print(self.df[target_column].value_counts())\n",
        "        else:\n",
        "            print(f\"Hata: '{target_column}' sütunu bulunamadı.\")\n",
        "\n",
        "    def remove_candidates(self, target_column: str) -> None:\n",
        "        if target_column in self.df.columns:\n",
        "            self.df = self.df[self.df[target_column] != 'CANDIDATE']\n",
        "            print(\"CANDIDATE etiketli satırlar temizlendi.\")\n",
        "        else:\n",
        "            print(f\"Hata: '{target_column}' sütunu bulunamadı.\")\n"
      ],
      "metadata": {
        "id": "Wh3hSNIz0UJJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "2bM3iNqyyPrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExoplanetModelTrainer:\n",
        "\n",
        "    def __init__(self, dataframe: pd.DataFrame, target_column: str,\n",
        "                 use_statistical: bool = True, use_fourier: bool = True,\n",
        "                 use_wavelet: bool = True, use_manual: bool = True,\n",
        "                 feature_columns: list = ['koi_period', 'koi_depth', 'koi_impact', 'koi_duration', 'koi_steff', 'koi_srad'],\n",
        "                 test_size: float = 0.2, validation_size: float = 0.2, random_state: int = 42,\n",
        "                 hidden_layer_sizes: tuple = (100,), activation: str = 'relu', solver: str = 'adam', learning_rate: str = 'constant',\n",
        "                 learning_rate_init: float = 0.001, max_iter: int = 300, output_dir: str = \"../reports\"):\n",
        "\n",
        "        self.df = dataframe\n",
        "        self.target_column = target_column\n",
        "        self.use_statistical = use_statistical\n",
        "        self.use_fourier = use_fourier\n",
        "        self.use_wavelet = use_wavelet\n",
        "        self.use_manual = use_manual\n",
        "        self.feature_columns = feature_columns\n",
        "        self.test_size = test_size\n",
        "        self.validation_size = validation_size\n",
        "        self.random_state = random_state\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.activation = activation\n",
        "        self.solver = solver\n",
        "        self.learning_rate = learning_rate\n",
        "        self.learning_rate_init = learning_rate_init\n",
        "        self.max_iter = max_iter\n",
        "        self.output_dir = output_dir\n",
        "        self.timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.run_dir = os.path.join(self.output_dir, self.timestamp)\n",
        "        os.makedirs(self.run_dir, exist_ok=True)\n",
        "        self.X_train, self.X_val, self.X_test, self.y_train, self.y_val, self.y_test = self._prepare_data()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_val_scaled = self.scaler.transform(self.X_val)\n",
        "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
        "        self.model = MLPClassifier(random_state=self.random_state, hidden_layer_sizes=self.hidden_layer_sizes,\n",
        "                                    activation=self.activation, solver=self.solver, learning_rate=self.learning_rate,\n",
        "                                    learning_rate_init=self.learning_rate_init, max_iter=self.max_iter)\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "        self.test_accuracies = []\n",
        "\n",
        "    def _prepare_data(self) -> tuple:\n",
        "        X = pd.DataFrame(index=self.df.df.index)\n",
        "\n",
        "        if self.use_statistical:\n",
        "            X = X.join(self.df.extract_statistical_features(self.feature_columns))\n",
        "        if self.use_fourier:\n",
        "            X = X.join(self.df.extract_fourier_features(self.feature_columns))\n",
        "        if self.use_wavelet:\n",
        "            X = X.join(self.df.extract_wavelet_features(self.feature_columns))\n",
        "        if self.use_manual:\n",
        "            X = X.join(self.df.extract_manual_features())\n",
        "\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X = imputer.fit_transform(X)\n",
        "\n",
        "        y = self.df.df[self.target_column]\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, shuffle=False)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=self.validation_size / (1 - self.test_size), random_state=self.random_state, shuffle = False)\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "        for epoch in range(self.max_iter):\n",
        "            self.model.partial_fit(self.X_train_scaled, self.y_train, classes=pd.unique(self.y_train))  # classes parametresi eklendi\n",
        "\n",
        "            y_train_pred = self.model.predict(self.X_train_scaled)\n",
        "            train_accuracy = accuracy_score(self.y_train, y_train_pred)\n",
        "            self.train_accuracies.append(train_accuracy)\n",
        "\n",
        "            y_val_pred = self.model.predict(self.X_val_scaled)\n",
        "            val_accuracy = accuracy_score(self.y_val, y_val_pred)\n",
        "            self.val_accuracies.append(val_accuracy)\n",
        "\n",
        "            y_test_pred = self.model.predict(self.X_test_scaled)\n",
        "            test_accuracy = accuracy_score(self.y_test, y_test_pred)\n",
        "            self.test_accuracies.append(test_accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.max_iter}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    def evaluate_train_set(self):\n",
        "        y_pred = self.model.predict(self.X_train_scaled)\n",
        "        accuracy = accuracy_score(self.y_train, y_pred)\n",
        "        report = classification_report(self.y_train, y_pred)\n",
        "        cm = confusion_matrix(self.y_train, y_pred)\n",
        "        return accuracy, report, cm\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        y_pred = self.model.predict(self.X_test_scaled)\n",
        "        accuracy = accuracy_score(self.y_test, y_pred)\n",
        "        report = classification_report(self.y_test, y_pred)\n",
        "        cm = confusion_matrix(self.y_test, y_pred)\n",
        "        return accuracy, report, cm\n",
        "\n",
        "    def evaluate_validation_set(self):\n",
        "        y_pred = self.model.predict(self.X_val_scaled)\n",
        "        accuracy = accuracy_score(self.y_val, y_pred)\n",
        "        report = classification_report(self.y_val, y_pred)\n",
        "        cm = confusion_matrix(self.y_val, y_pred)\n",
        "        return accuracy, report, cm\n",
        "\n",
        "    def save_results(self, test_accuracy: float, test_report: str, test_cm: list, val_accuracy: float, val_report: str, val_cm: list, train_accuracy: float, train_report: str, train_cm: list):         # Metin dosyasına model yapısını ve sonuçları kaydet\n",
        "        model_summary_path = os.path.join(self.run_dir, \"model_summary.txt\")\n",
        "        with open(model_summary_path, \"w\") as f:\n",
        "            f.write(f\"Model Yapısı:\\n\")\n",
        "            f.write(f\"Hidden Layer Sizes: {self.hidden_layer_sizes}\\n\")\n",
        "            f.write(f\"Activation Function: {self.activation}\\n\")\n",
        "            f.write(f\"Solver: {self.solver}\\n\")\n",
        "            f.write(f\"Learning Rate: {self.learning_rate}\\n\")\n",
        "            f.write(f\"Learning Rate Init: {self.learning_rate_init}\\n\")\n",
        "            f.write(f\"Max Iterations: {self.max_iter}\\n\")\n",
        "            f.write(f\"\\nSon Epoch Train Seti Doğruluğu: {self.train_accuracies[-1]}\\n\")\n",
        "            f.write(f\"\\nSon Epoch Test Seti Doğruluğu: {test_accuracy}\\n\")\n",
        "            f.write(f\"Test Seti Sınıflandırma Raporu:\\n{test_report}\\n\")\n",
        "            f.write(f\"\\nSon Epoch Validation Seti Doğruluğu: {val_accuracy}\\n\")\n",
        "            f.write(f\"Validation Seti Sınıflandırma Raporu:\\n{val_report}\\n\")\n",
        "            f.write(f\"Test Size: {self.test_size}\\n\")\n",
        "            f.write(f\"Validation Size: {self.validation_size}\\n\")\n",
        "            f.write(f\"Random State: {self.random_state}\\n\")\n",
        "            f.write(f\"\\nSon Epoch Train Seti Doğruluğu: {self.train_accuracies[-1]}\\n\")\n",
        "            f.write(f\"\\nSon Epoch Test Seti Doğruluğu: {self.test_accuracies[-1]}\\n\")\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(self.train_accuracies, label='Train Accuracy', color='red')\n",
        "        plt.plot(self.val_accuracies, label='Validation Accuracy', color='green')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Doğruluk')\n",
        "        plt.title('Epoch\\'a Göre Train ve Validation Doğruluğu')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        accuracy_plot_path = os.path.join(self.run_dir, \"accuracy_vs_epoch.png\")\n",
        "        plt.savefig(accuracy_plot_path)\n",
        "        plt.plot(self.test_accuracies, label=\"Test Accuracy\", color=\"blue\") #test verisi eklendi\n",
        "        plt.title('Epoch\\'a Göre Train, Validation ve Test Doğruluğu')\n",
        "        plt.close()\n",
        "\n",
        "        self._plot_confusion_matrix(test_cm, \"Test Confusion Matrix\", os.path.join(self.run_dir, \"test_confusion_matrix.png\"))\n",
        "        self._plot_confusion_matrix(val_cm, \"Validation Confusion Matrix\", os.path.join(self.run_dir, \"validation_confusion_matrix.png\"))\n",
        "        self._plot_confusion_matrix(train_cm, \"Train Confusion Matrix\", os.path.join(self.run_dir, \"train_confusion_matrix.png\"))\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm: list, title: str, filepath: str):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "        plt.title(title)\n",
        "        plt.ylabel('Gerçek Etiketler')\n",
        "        plt.xlabel('Tahmin Edilen Etiketler')\n",
        "        plt.savefig(filepath)\n",
        "        plt.close()\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def get_test_data(self):\n",
        "        return self.X_test_scaled, self.y_test"
      ],
      "metadata": {
        "id": "iPLELKuZyQQb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data & Train Model"
      ],
      "metadata": {
        "id": "Cr1lAqjK0-nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  exoplanet_data = ExoplanetData(EXOPLANET_DATA_FILE)\n",
        "  target_column = 'koi_disposition'\n",
        "  exoplanet_data.remove_candidates(target_column)\n",
        "\n",
        "  #feature_columns = ['koi_period', 'koi_depth', 'koi_impact', 'koi_duration', 'koi_steff', 'koi_srad']\n",
        "\n",
        "  feature_columns = [\n",
        "          'koi_period',      # Yörünge periyodu (gezegenin yıldız etrafında dönme süresi)\n",
        "          'koi_duration',    # Geçiş süresi (gezegenin yıldızın önünden geçme süresi)\n",
        "          'koi_depth',       # Geçiş derinliği (yıldızın parlaklığındaki azalma miktarı)\n",
        "          'koi_impact',      # Geçişin etkisi (gezegenin yıldızın merkezinden ne kadar uzakta geçtiği)\n",
        "          'koi_ror',         # Gezegenin yarıçapının yıldızın yarıçapına oranı\n",
        "          'koi_teq',         # Gezegenin denge sıcaklığı\n",
        "          'koi_model_snr',   # Model uyum sinyal-gürültü oranı (geçiş sinyalinin ne kadar belirgin olduğu)\n",
        "          'koi_steff',       # Yıldızın etkin sıcaklığı\n",
        "          'koi_slogg',       # Yıldızın yüzey çekimi\n",
        "          'koi_srad'         # Yıldızın yarıçapı\n",
        "  ]\n",
        "\n",
        "  data = exoplanet_data.get_dataframe_with_columns([target_column] + feature_columns)\n",
        "  data = data.dropna()\n",
        "\n",
        "  exoplanet_data.show_target_distribution(target_column)\n",
        "\n",
        "  data[target_column] = data[target_column].astype('category').cat.codes\n",
        "\n",
        "  trainer = ExoplanetModelTrainer(dataframe=exoplanet_data,\n",
        "                                  target_column=target_column,\n",
        "                                  use_statistical=True,\n",
        "                                  use_fourier=True,\n",
        "                                  use_wavelet=True,\n",
        "                                  use_manual=True,\n",
        "                                  feature_columns=feature_columns,\n",
        "                                  test_size=0.2, validation_size=0.2, random_state=42,\n",
        "                                  hidden_layer_sizes=(128, 64),\n",
        "                                  activation='relu',\n",
        "                                  solver='adam',\n",
        "                                  learning_rate='adaptive',\n",
        "                                  learning_rate_init=0.001,\n",
        "                                  max_iter=50,\n",
        "                                  output_dir=\"/content/drive/My Drive/Startrace_Project_Data/logs\")\n",
        "\n",
        "  trainer.train_model()\n",
        "\n",
        "  accuracy, report, test_cm = trainer.evaluate_model()\n",
        "  val_accuracy, val_report, val_cm = trainer.evaluate_validation_set()\n",
        "  train_accuracy, train_report, train_cm = trainer.evaluate_train_set()\n",
        "\n",
        "  trainer.save_results(accuracy, report, test_cm, val_accuracy, val_report, val_cm, train_accuracy, train_report, train_cm)\n",
        "\n",
        "  print(f\"Model Doğruluğu (Train Seti): {train_accuracy}\")\n",
        "  print(f\"Model Doğruluğu (Test Seti): {accuracy}\")\n",
        "  print(\"Sınıflandırma Raporu (Test Seti):\\n\", report)\n",
        "  print(f\"Model Doğruluğu (Validation Seti): {val_accuracy}\")\n",
        "  print(\"Sınıflandırma Raporu (Validation Seti):\\n\", val_report)\n",
        "  print(f\"Raporlar {trainer.run_dir} dizinine kaydedildi.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(f\"{EXOPLANET_DATA_FILE} dosyası bulunamadı.\")\n",
        "except KeyError as e:\n",
        "  print(f\"Sütun hatası: {e}. Lütfen sütun adlarını kontrol edin.\")\n",
        "except Exception as e:\n",
        "  print(f\"Beklenmedik bir hata oluştu: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r96wtSRA1Cqm",
        "outputId": "bb037f05-3f66-4090-bdae-f2995452ce24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CANDIDATE etiketli satırlar temizlendi.\n",
            "koi_disposition\n",
            "FALSE POSITIVE    4839\n",
            "CONFIRMED         2743\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['koi_depth_fft_mean' 'koi_depth_fft_std' 'koi_depth_fft_max'\n",
            " 'koi_impact_fft_mean' 'koi_impact_fft_std' 'koi_impact_fft_max'\n",
            " 'koi_ror_fft_mean' 'koi_ror_fft_std' 'koi_ror_fft_max' 'koi_teq_fft_mean'\n",
            " 'koi_teq_fft_std' 'koi_teq_fft_max' 'koi_model_snr_fft_mean'\n",
            " 'koi_model_snr_fft_std' 'koi_model_snr_fft_max' 'koi_steff_fft_mean'\n",
            " 'koi_steff_fft_std' 'koi_steff_fft_max' 'koi_slogg_fft_mean'\n",
            " 'koi_slogg_fft_std' 'koi_slogg_fft_max' 'koi_srad_fft_mean'\n",
            " 'koi_srad_fft_std' 'koi_srad_fft_max' 'koi_depth_wavelet_detail_mean'\n",
            " 'koi_depth_wavelet_detail_std' 'koi_depth_wavelet_approx_mean'\n",
            " 'koi_depth_wavelet_approx_std' 'koi_impact_wavelet_detail_mean'\n",
            " 'koi_impact_wavelet_detail_std' 'koi_impact_wavelet_approx_mean'\n",
            " 'koi_impact_wavelet_approx_std' 'koi_ror_wavelet_detail_mean'\n",
            " 'koi_ror_wavelet_detail_std' 'koi_ror_wavelet_approx_mean'\n",
            " 'koi_ror_wavelet_approx_std' 'koi_teq_wavelet_detail_mean'\n",
            " 'koi_teq_wavelet_detail_std' 'koi_teq_wavelet_approx_mean'\n",
            " 'koi_teq_wavelet_approx_std' 'koi_model_snr_wavelet_detail_mean'\n",
            " 'koi_model_snr_wavelet_detail_std' 'koi_model_snr_wavelet_approx_mean'\n",
            " 'koi_model_snr_wavelet_approx_std' 'koi_steff_wavelet_detail_mean'\n",
            " 'koi_steff_wavelet_detail_std' 'koi_steff_wavelet_approx_mean'\n",
            " 'koi_steff_wavelet_approx_std' 'koi_slogg_wavelet_detail_mean'\n",
            " 'koi_slogg_wavelet_detail_std' 'koi_slogg_wavelet_approx_mean'\n",
            " 'koi_slogg_wavelet_approx_std' 'koi_srad_wavelet_detail_mean'\n",
            " 'koi_srad_wavelet_detail_std' 'koi_srad_wavelet_approx_mean'\n",
            " 'koi_srad_wavelet_approx_std']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Accuracy: 0.6401, Validation Accuracy: 0.4199, Test Accuracy: 0.5208\n",
            "Epoch 2/50, Train Accuracy: 0.6489, Validation Accuracy: 0.3856, Test Accuracy: 0.4964\n",
            "Epoch 3/50, Train Accuracy: 0.6645, Validation Accuracy: 0.4904, Test Accuracy: 0.5557\n",
            "Epoch 4/50, Train Accuracy: 0.6689, Validation Accuracy: 0.4957, Test Accuracy: 0.5636\n",
            "Epoch 5/50, Train Accuracy: 0.6744, Validation Accuracy: 0.4911, Test Accuracy: 0.5702\n",
            "Epoch 6/50, Train Accuracy: 0.6774, Validation Accuracy: 0.4852, Test Accuracy: 0.5669\n",
            "Epoch 7/50, Train Accuracy: 0.6794, Validation Accuracy: 0.4832, Test Accuracy: 0.5669\n",
            "Epoch 8/50, Train Accuracy: 0.6814, Validation Accuracy: 0.4871, Test Accuracy: 0.5689\n",
            "Epoch 9/50, Train Accuracy: 0.6847, Validation Accuracy: 0.4878, Test Accuracy: 0.5735\n",
            "Epoch 10/50, Train Accuracy: 0.6867, Validation Accuracy: 0.4951, Test Accuracy: 0.5768\n",
            "Epoch 11/50, Train Accuracy: 0.6887, Validation Accuracy: 0.4970, Test Accuracy: 0.5808\n",
            "Epoch 12/50, Train Accuracy: 0.6895, Validation Accuracy: 0.4984, Test Accuracy: 0.5840\n",
            "Epoch 13/50, Train Accuracy: 0.6909, Validation Accuracy: 0.5003, Test Accuracy: 0.5854\n",
            "Epoch 14/50, Train Accuracy: 0.6920, Validation Accuracy: 0.5010, Test Accuracy: 0.5867\n",
            "Epoch 15/50, Train Accuracy: 0.6944, Validation Accuracy: 0.5036, Test Accuracy: 0.5887\n",
            "Epoch 16/50, Train Accuracy: 0.6961, Validation Accuracy: 0.5063, Test Accuracy: 0.5887\n",
            "Epoch 17/50, Train Accuracy: 0.6979, Validation Accuracy: 0.5069, Test Accuracy: 0.5920\n",
            "Epoch 18/50, Train Accuracy: 0.6985, Validation Accuracy: 0.5082, Test Accuracy: 0.5926\n",
            "Epoch 19/50, Train Accuracy: 0.6994, Validation Accuracy: 0.5089, Test Accuracy: 0.5933\n",
            "Epoch 20/50, Train Accuracy: 0.7001, Validation Accuracy: 0.5089, Test Accuracy: 0.5933\n",
            "Epoch 21/50, Train Accuracy: 0.7010, Validation Accuracy: 0.5089, Test Accuracy: 0.5946\n",
            "Epoch 22/50, Train Accuracy: 0.7012, Validation Accuracy: 0.5109, Test Accuracy: 0.5953\n",
            "Epoch 23/50, Train Accuracy: 0.7014, Validation Accuracy: 0.5115, Test Accuracy: 0.5959\n",
            "Epoch 24/50, Train Accuracy: 0.7016, Validation Accuracy: 0.5102, Test Accuracy: 0.5959\n",
            "Epoch 25/50, Train Accuracy: 0.7025, Validation Accuracy: 0.5135, Test Accuracy: 0.5979\n",
            "Epoch 26/50, Train Accuracy: 0.7023, Validation Accuracy: 0.5129, Test Accuracy: 0.5979\n",
            "Epoch 27/50, Train Accuracy: 0.7034, Validation Accuracy: 0.5148, Test Accuracy: 0.5992\n",
            "Epoch 28/50, Train Accuracy: 0.7040, Validation Accuracy: 0.5142, Test Accuracy: 0.6005\n",
            "Epoch 29/50, Train Accuracy: 0.7040, Validation Accuracy: 0.5148, Test Accuracy: 0.6018\n",
            "Epoch 30/50, Train Accuracy: 0.7047, Validation Accuracy: 0.5155, Test Accuracy: 0.6025\n",
            "Epoch 31/50, Train Accuracy: 0.7051, Validation Accuracy: 0.5155, Test Accuracy: 0.6025\n",
            "Epoch 32/50, Train Accuracy: 0.7051, Validation Accuracy: 0.5155, Test Accuracy: 0.6032\n",
            "Epoch 33/50, Train Accuracy: 0.7054, Validation Accuracy: 0.5168, Test Accuracy: 0.6032\n",
            "Epoch 34/50, Train Accuracy: 0.7065, Validation Accuracy: 0.5175, Test Accuracy: 0.6032\n",
            "Epoch 35/50, Train Accuracy: 0.7065, Validation Accuracy: 0.5175, Test Accuracy: 0.6032\n",
            "Epoch 36/50, Train Accuracy: 0.7069, Validation Accuracy: 0.5181, Test Accuracy: 0.6038\n",
            "Epoch 37/50, Train Accuracy: 0.7076, Validation Accuracy: 0.5175, Test Accuracy: 0.6045\n",
            "Epoch 38/50, Train Accuracy: 0.7080, Validation Accuracy: 0.5181, Test Accuracy: 0.6058\n",
            "Epoch 39/50, Train Accuracy: 0.7082, Validation Accuracy: 0.5201, Test Accuracy: 0.6051\n",
            "Epoch 40/50, Train Accuracy: 0.7087, Validation Accuracy: 0.5201, Test Accuracy: 0.6051\n",
            "Epoch 41/50, Train Accuracy: 0.7095, Validation Accuracy: 0.5201, Test Accuracy: 0.6051\n",
            "Epoch 42/50, Train Accuracy: 0.7100, Validation Accuracy: 0.5201, Test Accuracy: 0.6058\n",
            "Epoch 43/50, Train Accuracy: 0.7102, Validation Accuracy: 0.5208, Test Accuracy: 0.6065\n",
            "Epoch 44/50, Train Accuracy: 0.7115, Validation Accuracy: 0.5208, Test Accuracy: 0.6071\n",
            "Epoch 45/50, Train Accuracy: 0.7124, Validation Accuracy: 0.5214, Test Accuracy: 0.6078\n",
            "Epoch 46/50, Train Accuracy: 0.7124, Validation Accuracy: 0.5221, Test Accuracy: 0.6084\n",
            "Epoch 47/50, Train Accuracy: 0.7131, Validation Accuracy: 0.5221, Test Accuracy: 0.6084\n",
            "Epoch 48/50, Train Accuracy: 0.7124, Validation Accuracy: 0.5221, Test Accuracy: 0.6084\n",
            "Epoch 49/50, Train Accuracy: 0.7133, Validation Accuracy: 0.5221, Test Accuracy: 0.6084\n",
            "Epoch 50/50, Train Accuracy: 0.7142, Validation Accuracy: 0.5234, Test Accuracy: 0.6084\n",
            "Model Doğruluğu (Train Seti): 0.714160070360598\n",
            "Model Doğruluğu (Test Seti): 0.6084377059986816\n",
            "Sınıflandırma Raporu (Test Seti):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     CONFIRMED       0.01      0.82      0.03        11\n",
            "FALSE POSITIVE       1.00      0.61      0.75      1506\n",
            "\n",
            "      accuracy                           0.61      1517\n",
            "     macro avg       0.51      0.71      0.39      1517\n",
            "  weighted avg       0.99      0.61      0.75      1517\n",
            "\n",
            "Model Doğruluğu (Validation Seti): 0.5234014502307185\n",
            "Sınıflandırma Raporu (Validation Seti):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     CONFIRMED       0.14      0.85      0.23       131\n",
            "FALSE POSITIVE       0.97      0.49      0.65      1386\n",
            "\n",
            "      accuracy                           0.52      1517\n",
            "     macro avg       0.55      0.67      0.44      1517\n",
            "  weighted avg       0.90      0.52      0.62      1517\n",
            "\n",
            "Raporlar /content/drive/My Drive/Startrace_Project_Data/logs/20250316_120921 dizinine kaydedildi.\n"
          ]
        }
      ]
    }
  ]
}